{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desenvolvimento e Comparação de Modelos - Hackathon Forecast 2025\n",
    "\n",
    "Este notebook implementa, treina e compara diferentes modelos de machine learning para previsão de vendas, incluindo XGBoost, LightGBM, Prophet e estratégias de ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Modelos de ML\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from prophet import Prophet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Otimização de hiperparâmetros\n",
    "from optuna import create_study, Trial\n",
    "import optuna.visualization as vis\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Importar módulos do projeto\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from models.training import ModelTrainer\n",
    "from models.ensemble import EnsembleModel\n",
    "from models.validation import ModelValidator\n",
    "from utils.experiment_tracker import ExperimentTracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento dos Dados Processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar features processadas\n",
    "print(\"Carregando features processadas...\")\n",
    "features_df = pd.read_parquet('../data/processed/features_engineered.parquet')\n",
    "feature_importance = pd.read_csv('../data/processed/feature_importance.csv')\n",
    "selected_features = pd.read_csv('../data/processed/selected_features.csv')['feature'].tolist()\n",
    "\n",
    "print(f\"Dados carregados: {features_df.shape}\")\n",
    "print(f\"Features selecionadas: {len(selected_features)}\")\n",
    "\n",
    "# Remover NaN para treinamento\n",
    "train_data = features_df.dropna()\n",
    "print(f\"Dados para treinamento: {train_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados para modelagem\n",
    "# Usar apenas features numéricas\n",
    "numeric_features = train_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'quantidade' in numeric_features:\n",
    "    numeric_features.remove('quantidade')\n",
    "\n",
    "# Usar features selecionadas que estão disponíveis\n",
    "available_selected = [f for f in selected_features if f in numeric_features]\n",
    "print(f\"Features selecionadas disponíveis: {len(available_selected)}\")\n",
    "\n",
    "X = train_data[available_selected]\n",
    "y = train_data['quantidade']\n",
    "\n",
    "print(f\"Shape final: X={X.shape}, y={y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuração da Validação Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar validação cruzada temporal\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Visualizar splits temporais\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "    ax.barh(i, len(train_idx), left=0, height=0.3, \n",
    "            color='blue', alpha=0.7, label='Treino' if i == 0 else \"\")\n",
    "    ax.barh(i, len(val_idx), left=len(train_idx), height=0.3, \n",
    "            color='red', alpha=0.7, label='Validação' if i == 0 else \"\")\n",
    "\n",
    "ax.set_xlabel('Índice dos Dados')\n",
    "ax.set_ylabel('Fold')\n",
    "ax.set_title('Divisão Temporal para Validação Cruzada')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Configuração: {tscv.n_splits} folds temporais\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Métricas de Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calcular métricas de avaliação\"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    # WMAPE (Weighted Mean Absolute Percentage Error)\n",
    "    wmape = np.sum(np.abs(y_true - y_pred)) / np.sum(y_true) * 100\n",
    "    \n",
    "    # MAPE (Mean Absolute Percentage Error)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    return {\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'WMAPE': wmape,\n",
    "        'MAPE': mape\n",
    "    }\n",
    "\n",
    "def cross_validate_model(model, X, y, cv):\n",
    "    \"\"\"Validação cruzada temporal\"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        metrics = calculate_metrics(y_val, y_pred)\n",
    "        scores.append(metrics)\n",
    "    \n",
    "    # Calcular médias\n",
    "    avg_scores = {}\n",
    "    for metric in scores[0].keys():\n",
    "        avg_scores[f'{metric}_mean'] = np.mean([s[metric] for s in scores])\n",
    "        avg_scores[f'{metric}_std'] = np.std([s[metric] for s in scores])\n",
    "    \n",
    "    return avg_scores, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelo XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar XGBoost com parâmetros padrão\n",
    "print(\"Treinando XGBoost...\")\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_scores, xgb_fold_scores = cross_validate_model(xgb_model, X, y, tscv)\n",
    "\n",
    "print(\"Resultados XGBoost:\")\n",
    "for metric, value in xgb_scores.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modelo LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar LightGBM\n",
    "print(\"Treinando LightGBM...\")\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgb_scores, lgb_fold_scores = cross_validate_model(lgb_model, X, y, tscv)\n",
    "\n",
    "print(\"Resultados LightGBM:\")\n",
    "for metric, value in lgb_scores.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modelo Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar Random Forest\n",
    "print(\"Treinando Random Forest...\")\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_scores, rf_fold_scores = cross_validate_model(rf_model, X, y, tscv)\n",
    "\n",
    "print(\"Resultados Random Forest:\")\n",
    "for metric, value in rf_scores.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparação de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar resultados\n",
    "results_comparison = pd.DataFrame({\n",
    "    'Modelo': ['XGBoost', 'LightGBM', 'Random Forest'],\n",
    "    'WMAPE_mean': [xgb_scores['WMAPE_mean'], lgb_scores['WMAPE_mean'], rf_scores['WMAPE_mean']],\n",
    "    'WMAPE_std': [xgb_scores['WMAPE_std'], lgb_scores['WMAPE_std'], rf_scores['WMAPE_std']],\n",
    "    'MAE_mean': [xgb_scores['MAE_mean'], lgb_scores['MAE_mean'], rf_scores['MAE_mean']],\n",
    "    'RMSE_mean': [xgb_scores['RMSE_mean'], lgb_scores['RMSE_mean'], rf_scores['RMSE_mean']]\n",
    "})\n",
    "\n",
    "print(\"Comparação de Modelos:\")\n",
    "print(results_comparison.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar comparação\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('WMAPE', 'MAE', 'RMSE', 'Comparação Geral')\n",
    ")\n",
    "\n",
    "# WMAPE\n",
    "fig.add_trace(\n",
    "    go.Bar(x=results_comparison['Modelo'], y=results_comparison['WMAPE_mean'], \n",
    "           error_y=dict(type='data', array=results_comparison['WMAPE_std']),\n",
    "           name='WMAPE'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# MAE\n",
    "fig.add_trace(\n",
    "    go.Bar(x=results_comparison['Modelo'], y=results_comparison['MAE_mean'], name='MAE'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# RMSE\n",
    "fig.add_trace(\n",
    "    go.Bar(x=results_comparison['Modelo'], y=results_comparison['RMSE_mean'], name='RMSE'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Ranking geral (menor WMAPE é melhor)\n",
    "ranking = results_comparison.sort_values('WMAPE_mean')\n",
    "fig.add_trace(\n",
    "    go.Bar(x=ranking['Modelo'], y=ranking['WMAPE_mean'], name='Ranking'),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"Comparação de Performance dos Modelos\", showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Otimização de Hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otimização do melhor modelo (assumindo XGBoost)\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    scores, _ = cross_validate_model(model, X, y, tscv)\n",
    "    \n",
    "    return scores['WMAPE_mean']\n",
    "\n",
    "# Executar otimização (versão reduzida para demonstração)\n",
    "print(\"Otimizando hiperparâmetros do XGBoost...\")\n",
    "study = create_study(direction='minimize')\n",
    "study.optimize(objective_xgb, n_trials=20)  # Reduzido para demonstração\n",
    "\n",
    "print(f\"Melhor WMAPE: {study.best_value:.4f}\")\n",
    "print(\"Melhores parâmetros:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar modelo otimizado\n",
    "best_xgb = xgb.XGBRegressor(**study.best_params)\n",
    "best_xgb_scores, _ = cross_validate_model(best_xgb, X, y, tscv)\n",
    "\n",
    "print(\"Resultados XGBoost Otimizado:\")\n",
    "for metric, value in best_xgb_scores.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Comparar com versão original\n",
    "improvement = xgb_scores['WMAPE_mean'] - best_xgb_scores['WMAPE_mean']\n",
    "print(f\"\\nMelhoria no WMAPE: {improvement:.4f} ({improvement/xgb_scores['WMAPE_mean']*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ensemble de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar ensemble simples (média ponderada)\n",
    "def ensemble_predict(models, weights, X):\n",
    "    predictions = np.zeros(len(X))\n",
    "    \n",
    "    for model, weight in zip(models, weights):\n",
    "        pred = model.predict(X)\n",
    "        predictions += weight * pred\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Treinar modelos individuais no conjunto completo\n",
    "print(\"Treinando ensemble...\")\n",
    "\n",
    "# Modelos base\n",
    "xgb_final = xgb.XGBRegressor(**study.best_params)\n",
    "lgb_final = lgb.LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, \n",
    "                             random_state=42, n_jobs=-1, verbose=-1)\n",
    "rf_final = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "models = [xgb_final, lgb_final, rf_final]\n",
    "\n",
    "# Pesos baseados na performance (inverso do WMAPE)\n",
    "wmape_scores = [best_xgb_scores['WMAPE_mean'], lgb_scores['WMAPE_mean'], rf_scores['WMAPE_mean']]\n",
    "weights = [1/score for score in wmape_scores]\n",
    "weights = [w/sum(weights) for w in weights]  # Normalizar\n",
    "\n",
    "print(f\"Pesos do ensemble: {[f'{w:.3f}' for w in weights]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validar ensemble\n",
    "ensemble_scores = []\n",
    "\n",
    "for train_idx, val_idx in tscv.split(X):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Treinar modelos\n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predição ensemble\n",
    "    y_pred = ensemble_predict(models, weights, X_val)\n",
    "    \n",
    "    metrics = calculate_metrics(y_val, y_pred)\n",
    "    ensemble_scores.append(metrics)\n",
    "\n",
    "# Calcular médias\n",
    "ensemble_avg = {}\n",
    "for metric in ensemble_scores[0].keys():\n",
    "    ensemble_avg[f'{metric}_mean'] = np.mean([s[metric] for s in ensemble_scores])\n",
    "    ensemble_avg[f'{metric}_std'] = np.std([s[metric] for s in ensemble_scores])\n",
    "\n",
    "print(\"Resultados Ensemble:\")\n",
    "for metric, value in ensemble_avg.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Análise de Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar modelo final para análise de importância\n",
    "xgb_final.fit(X, y)\n",
    "\n",
    "# Feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': xgb_final.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Visualizar top 20\n",
    "top_20_importance = importance_df.head(20)\n",
    "\n",
    "fig = px.bar(\n",
    "    top_20_importance,\n",
    "    x='importance',\n",
    "    y='feature',\n",
    "    orientation='h',\n",
    "    title=\"Top 20 Features - Importância no Modelo Final\"\n",
    ")\n",
    "fig.update_layout(height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Análise de Resíduos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de resíduos do melhor modelo\n",
    "y_pred_final = xgb_final.predict(X)\n",
    "residuals = y - y_pred_final\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Predito vs Real', 'Distribuição dos Resíduos', \n",
    "                   'Resíduos vs Predito', 'Q-Q Plot')\n",
    ")\n",
    "\n",
    "# Predito vs Real\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=y, y=y_pred_final, mode='markers', name='Predições', opacity=0.6),\n",
    "    row=1, col=1\n",
    ")\n",
    "# Linha diagonal\n",
    "min_val, max_val = min(y.min(), y_pred_final.min()), max(y.max(), y_pred_final.max())\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=[min_val, max_val], y=[min_val, max_val], \n",
    "              mode='lines', name='Linha Ideal', line=dict(color='red')),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Distribuição dos resíduos\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=residuals, name='Resíduos', nbinsx=50),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Resíduos vs Predito\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=y_pred_final, y=residuals, mode='markers', name='Resíduos vs Predito', opacity=0.6),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"Análise de Resíduos - Modelo XGBoost\", showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "# Estatísticas dos resíduos\n",
    "print(f\"Estatísticas dos Resíduos:\")\n",
    "print(f\"Média: {residuals.mean():.4f}\")\n",
    "print(f\"Desvio Padrão: {residuals.std():.4f}\")\n",
    "print(f\"Mediana: {residuals.median():.4f}\")\n",
    "print(f\"MAE: {np.abs(residuals).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Resumo Final e Recomendações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar resultados finais\n",
    "final_results = pd.DataFrame({\n",
    "    'Modelo': ['XGBoost Original', 'XGBoost Otimizado', 'LightGBM', 'Random Forest', 'Ensemble'],\n",
    "    'WMAPE': [xgb_scores['WMAPE_mean'], best_xgb_scores['WMAPE_mean'], \n",
    "             lgb_scores['WMAPE_mean'], rf_scores['WMAPE_mean'], ensemble_avg['WMAPE_mean']],\n",
    "    'MAE': [xgb_scores['MAE_mean'], best_xgb_scores['MAE_mean'], \n",
    "           lgb_scores['MAE_mean'], rf_scores['MAE_mean'], ensemble_avg['MAE_mean']],\n",
    "    'RMSE': [xgb_scores['RMSE_mean'], best_xgb_scores['RMSE_mean'], \n",
    "            lgb_scores['RMSE_mean'], rf_scores['RMSE_mean'], ensemble_avg['RMSE_mean']]\n",
    "})\n",
    "\n",
    "final_results = final_results.sort_values('WMAPE')\n",
    "\n",
    "print(\"=== RESULTADOS FINAIS ===\")\n",
    "print(final_results.round(4))\n",
    "\n",
    "best_model = final_results.iloc[0]['Modelo']\n",
    "best_wmape = final_results.iloc[0]['WMAPE']\n",
    "\n",
    "print(f\"\\n=== MELHOR MODELO ===\")\n",
    "print(f\"Modelo: {best_model}\")\n",
    "print(f\"WMAPE: {best_wmape:.4f}%\")\n",
    "\n",
    "print(f\"\\n=== RECOMENDAÇÕES ===\")\n",
    "print(f\"1. Usar {best_model} como modelo principal\")\n",
    "print(f\"2. Considerar ensemble para maior robustez\")\n",
    "print(f\"3. Monitorar performance em dados novos\")\n",
    "print(f\"4. Ajustar hiperparâmetros conforme necessário\")\n",
    "print(f\"5. Implementar validação contínua\")\n",
    "\n",
    "print(f\"\\n=== PRÓXIMOS PASSOS ===\")\n",
    "print(f\"1. Treinar modelo final em todos os dados\")\n",
    "print(f\"2. Gerar previsões para janeiro/2023\")\n",
    "print(f\"3. Aplicar pós-processamento\")\n",
    "print(f\"4. Validar formato de submissão\")\n",
    "print(f\"5. Preparar múltiplas submissões\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar modelos e resultados\n",
    "import joblib\n",
    "\n",
    "print(\"Salvando modelos e resultados...\")\n",
    "\n",
    "# Salvar melhor modelo\n",
    "joblib.dump(xgb_final, '../models/best_xgb_model.pkl')\n",
    "joblib.dump(models, '../models/ensemble_models.pkl')\n",
    "\n",
    "# Salvar resultados\n",
    "final_results.to_csv('../data/processed/model_comparison_results.csv', index=False)\n",
    "importance_df.to_csv('../data/processed/final_feature_importance.csv', index=False)\n",
    "\n",
    "# Salvar configurações do melhor modelo\n",
    "best_config = {\n",
    "    'model_type': 'XGBoost',\n",
    "    'parameters': study.best_params,\n",
    "    'features': X.columns.tolist(),\n",
    "    'performance': {\n",
    "        'WMAPE': best_xgb_scores['WMAPE_mean'],\n",
    "        'MAE': best_xgb_scores['MAE_mean'],\n",
    "        'RMSE': best_xgb_scores['RMSE_mean']\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../configs/best_model_config.json', 'w') as f:\n",
    "    json.dump(best_config, f, indent=2)\n",
    "\n",
    "print(\"Modelos e resultados salvos com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}