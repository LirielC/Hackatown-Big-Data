{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An√°lise Explorat√≥ria de Dados (EDA) - Hackathon Forecast 2025\n",
    "\n",
    "Este notebook cont√©m a an√°lise explorat√≥ria completa dos dados de vendas para o modelo de previs√£o.\n",
    "\n",
    "## Objetivos:\n",
    "- Entender a estrutura e qualidade dos dados\n",
    "- Identificar padr√µes temporais e sazonalidade\n",
    "- Analisar distribui√ß√µes por categoria de produto e tipo de PDV\n",
    "- Identificar outliers e dados faltantes\n",
    "- Gerar insights para feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necess√°rios\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Adicionar src ao path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data.ingestion import DataIngestion\n",
    "from data.preprocessing import DataPreprocessor\n",
    "\n",
    "# Configura√ß√µes\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Configurar tamanho das figuras\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}  }
,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento e Primeira Inspe√ß√£o dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar classes de ingest√£o e preprocessamento\n",
    "ingestion = DataIngestion(use_polars=True)\n",
    "preprocessor = DataPreprocessor(use_polars=False)\n",
    "\n",
    "# Carregar dados dos arquivos Parquet\n",
    "data_path = Path('../hackathon_2025_templates')\n",
    "print(f\"Carregando dados de: {data_path}\")\n",
    "\n",
    "# Carregar m√∫ltiplos schemas se necess√°rio\n",
    "try:\n",
    "    data_schemas = ingestion.load_multiple_schemas(data_path, sample_only=True)\n",
    "    print(f\"\\nEncontrados {len(data_schemas)} schemas diferentes:\")\n",
    "    \n",
    "    for i, (schema, df) in enumerate(data_schemas.items()):\n",
    "        print(f\"\\nSchema {i+1}: {len(df)} registros\")\n",
    "        print(f\"Colunas: {list(schema)}\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        \n",
    "        # Mostrar primeiras linhas\n",
    "        display(df.head())\n",
    "        \n",
    "        # Informa√ß√µes b√°sicas\n",
    "        print(\"\\nInfo b√°sica:\")\n",
    "        print(df.info())\n",
    "        \n",
    "        # Salvar o maior dataset para an√°lise principal\n",
    "        if i == 0 or len(df) > len(main_df):\n",
    "            main_df = df.copy()\n",
    "            main_schema = schema\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar dados: {e}\")\n",
    "    # Fallback: tentar carregar arquivo por arquivo\n",
    "    parquet_files = list(data_path.glob(\"*.parquet\"))\n",
    "    main_df = pd.read_parquet(parquet_files[0])\n",
    "    print(f\"Carregado arquivo individual: {parquet_files[0].name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise inicial do dataset principal\n",
    "print(\"=== AN√ÅLISE INICIAL DO DATASET PRINCIPAL ===\")\n",
    "print(f\"Shape: {main_df.shape}\")\n",
    "print(f\"Colunas: {list(main_df.columns)}\")\n",
    "print(f\"Tipos de dados:\")\n",
    "print(main_df.dtypes)\n",
    "\n",
    "print(\"\\n=== PRIMEIRAS 10 LINHAS ===\")\n",
    "display(main_df.head(10))\n",
    "\n",
    "print(\"\\n=== √öLTIMAS 5 LINHAS ===\")\n",
    "display(main_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estat√≠sticas descritivas\n",
    "print(\"=== ESTAT√çSTICAS DESCRITIVAS ===\")\n",
    "\n",
    "# Para colunas num√©ricas\n",
    "numeric_cols = main_df.select_dtypes(include=[np.number]).columns\n",
    "if len(numeric_cols) > 0:\n",
    "    print(\"\\nColunas num√©ricas:\")\n",
    "    display(main_df[numeric_cols].describe())\n",
    "\n",
    "# Para colunas categ√≥ricas\n",
    "categorical_cols = main_df.select_dtypes(include=['object']).columns\n",
    "if len(categorical_cols) > 0:\n",
    "    print(\"\\nColunas categ√≥ricas:\")\n",
    "    for col in categorical_cols:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Valores √∫nicos: {main_df[col].nunique()}\")\n",
    "        print(f\"  Top 5 valores:\")\n",
    "        print(main_df[col].value_counts().head())"
   ]
  }  
},
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. An√°lise de Qualidade dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de valores faltantes\n",
    "print(\"=== AN√ÅLISE DE VALORES FALTANTES ===\")\n",
    "\n",
    "missing_data = main_df.isnull().sum()\n",
    "missing_percent = (missing_data / len(main_df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Coluna': missing_data.index,\n",
    "    'Valores_Faltantes': missing_data.values,\n",
    "    'Percentual': missing_percent.values\n",
    "})\n",
    "\n",
    "missing_df = missing_df[missing_df['Valores_Faltantes'] > 0].sort_values('Percentual', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    display(missing_df)\n",
    "    \n",
    "    # Visualiza√ß√£o de valores faltantes\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=missing_df, x='Coluna', y='Percentual')\n",
    "    plt.title('Percentual de Valores Faltantes por Coluna')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Percentual (%)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Nenhum valor faltante encontrado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de duplicatas\n",
    "print(\"=== AN√ÅLISE DE DUPLICATAS ===\")\n",
    "\n",
    "total_duplicates = main_df.duplicated().sum()\n",
    "duplicate_percent = (total_duplicates / len(main_df)) * 100\n",
    "\n",
    "print(f\"Total de registros duplicados: {total_duplicates} ({duplicate_percent:.2f}%)\")\n",
    "\n",
    "if total_duplicates > 0:\n",
    "    print(\"\\nPrimeiros registros duplicados:\")\n",
    "    duplicated_rows = main_df[main_df.duplicated(keep=False)]\n",
    "    display(duplicated_rows.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de tipos de dados e convers√µes necess√°rias\n",
    "print(\"=== AN√ÅLISE DE TIPOS DE DADOS ===\")\n",
    "\n",
    "# Identificar poss√≠veis colunas de data\n",
    "date_columns = [col for col in main_df.columns if any(keyword in col.lower() \n",
    "                for keyword in ['date', 'data', 'time', 'timestamp'])]\n",
    "\n",
    "print(f\"Poss√≠veis colunas de data: {date_columns}\")\n",
    "\n",
    "# Identificar poss√≠veis colunas de ID\n",
    "id_columns = [col for col in main_df.columns if any(keyword in col.lower() \n",
    "              for keyword in ['id', 'pdv', 'produto', 'store', 'product'])]\n",
    "\n",
    "print(f\"Poss√≠veis colunas de ID: {id_columns}\")\n",
    "\n",
    "# Identificar poss√≠veis colunas de quantidade/valor\n",
    "quantity_columns = [col for col in main_df.columns if any(keyword in col.lower() \n",
    "                   for keyword in ['quantidade', 'quantity', 'value', 'valor', 'sales'])]\n",
    "\n",
    "print(f\"Poss√≠veis colunas de quantidade/valor: {quantity_columns}\")\n",
    "\n",
    "# Mostrar estat√≠sticas de mem√≥ria\n",
    "memory_usage = main_df.memory_usage(deep=True)\n",
    "total_memory = memory_usage.sum() / 1024 / 1024  # MB\n",
    "print(f\"\\nUso total de mem√≥ria: {total_memory:.2f} MB\")"
   ]
  }  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. An√°lise Temporal e Sazonalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados para an√°lise temporal\n",
    "print(\"=== PREPARA√á√ÉO PARA AN√ÅLISE TEMPORAL ===\")\n",
    "\n",
    "# Tentar identificar e converter coluna de data\n",
    "df_temporal = main_df.copy()\n",
    "\n",
    "# Procurar coluna de data\n",
    "date_col = None\n",
    "for col in df_temporal.columns:\n",
    "    if any(keyword in col.lower() for keyword in ['date', 'data', 'time']):\n",
    "        try:\n",
    "            df_temporal[col] = pd.to_datetime(df_temporal[col])\n",
    "            date_col = col\n",
    "            print(f\"Coluna de data identificada: {col}\")\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "if date_col:\n",
    "    # Filtrar apenas dados de 2022\n",
    "    df_temporal = df_temporal[df_temporal[date_col].dt.year == 2022]\n",
    "    print(f\"Dados filtrados para 2022: {len(df_temporal)} registros\")\n",
    "    \n",
    "    # Criar features temporais\n",
    "    df_temporal['ano'] = df_temporal[date_col].dt.year\n",
    "    df_temporal['mes'] = df_temporal[date_col].dt.month\n",
    "    df_temporal['semana'] = df_temporal[date_col].dt.isocalendar().week\n",
    "    df_temporal['dia_semana'] = df_temporal[date_col].dt.dayofweek\n",
    "    df_temporal['trimestre'] = df_temporal[date_col].dt.quarter\n",
    "    \n",
    "    print(f\"Range de datas: {df_temporal[date_col].min()} a {df_temporal[date_col].max()}\")\n",
    "    print(f\"Total de semanas: {df_temporal['semana'].nunique()}\")\n",
    "else:\n",
    "    print(\"AVISO: Nenhuma coluna de data identificada!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de distribui√ß√£o temporal\n",
    "if date_col:\n",
    "    print(\"=== AN√ÅLISE DE DISTRIBUI√á√ÉO TEMPORAL ===\")\n",
    "    \n",
    "    # Identificar coluna de quantidade\n",
    "    qty_col = None\n",
    "    for col in df_temporal.columns:\n",
    "        if any(keyword in col.lower() for keyword in ['quantidade', 'quantity', 'sales']):\n",
    "            if df_temporal[col].dtype in ['int64', 'float64']:\n",
    "                qty_col = col\n",
    "                break\n",
    "    \n",
    "    if qty_col:\n",
    "        # Agrega√ß√£o por m√™s\n",
    "        monthly_sales = df_temporal.groupby('mes')[qty_col].agg(['sum', 'mean', 'count']).reset_index()\n",
    "        monthly_sales['mes_nome'] = monthly_sales['mes'].map({\n",
    "            1: 'Jan', 2: 'Fev', 3: 'Mar', 4: 'Abr', 5: 'Mai', 6: 'Jun',\n",
    "            7: 'Jul', 8: 'Ago', 9: 'Set', 10: 'Out', 11: 'Nov', 12: 'Dez'\n",
    "        })\n",
    "        \n",
    "        print(\"Vendas por m√™s:\")\n",
    "        display(monthly_sales)\n",
    "        \n",
    "        # Visualiza√ß√£o mensal\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Vendas totais por m√™s\n",
    "        axes[0,0].bar(monthly_sales['mes_nome'], monthly_sales['sum'])\n",
    "        axes[0,0].set_title('Vendas Totais por M√™s')\n",
    "        axes[0,0].set_ylabel('Quantidade Total')\n",
    "        axes[0,0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Vendas m√©dias por m√™s\n",
    "        axes[0,1].bar(monthly_sales['mes_nome'], monthly_sales['mean'])\n",
    "        axes[0,1].set_title('Vendas M√©dias por M√™s')\n",
    "        axes[0,1].set_ylabel('Quantidade M√©dia')\n",
    "        axes[0,1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # N√∫mero de transa√ß√µes por m√™s\n",
    "        axes[1,0].bar(monthly_sales['mes_nome'], monthly_sales['count'])\n",
    "        axes[1,0].set_title('N√∫mero de Transa√ß√µes por M√™s')\n",
    "        axes[1,0].set_ylabel('N√∫mero de Transa√ß√µes')\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Vendas por dia da semana\n",
    "        weekly_sales = df_temporal.groupby('dia_semana')[qty_col].sum()\n",
    "        dias_semana = ['Seg', 'Ter', 'Qua', 'Qui', 'Sex', 'S√°b', 'Dom']\n",
    "        axes[1,1].bar(dias_semana, weekly_sales.values)\n",
    "        axes[1,1].set_title('Vendas por Dia da Semana')\n",
    "        axes[1,1].set_ylabel('Quantidade Total')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"AVISO: Nenhuma coluna de quantidade identificada!\")"
   ]
  }  }
,
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de sazonalidade semanal\n",
    "if date_col and qty_col:\n",
    "    print(\"=== AN√ÅLISE DE SAZONALIDADE SEMANAL ===\")\n",
    "    \n",
    "    # Agrega√ß√£o por semana\n",
    "    weekly_data = df_temporal.groupby(['semana'])[qty_col].agg(['sum', 'mean', 'count']).reset_index()\n",
    "    \n",
    "    # Visualiza√ß√£o da s√©rie temporal semanal\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(weekly_data['semana'], weekly_data['sum'], marker='o', linewidth=2)\n",
    "    plt.title('Evolu√ß√£o das Vendas Semanais em 2022')\n",
    "    plt.xlabel('Semana do Ano')\n",
    "    plt.ylabel('Quantidade Total')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(weekly_data['semana'], weekly_data['mean'], marker='s', color='orange', linewidth=2)\n",
    "    plt.title('M√©dia de Vendas por Transa√ß√£o - Semanal')\n",
    "    plt.xlabel('Semana do Ano')\n",
    "    plt.ylabel('Quantidade M√©dia por Transa√ß√£o')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Estat√≠sticas de sazonalidade\n",
    "    print(f\"\\nEstat√≠sticas semanais:\")\n",
    "    print(f\"Semana com maior venda: {weekly_data.loc[weekly_data['sum'].idxmax(), 'semana']} (Quantidade: {weekly_data['sum'].max():,.0f})\")\n",
    "    print(f\"Semana com menor venda: {weekly_data.loc[weekly_data['sum'].idxmin(), 'semana']} (Quantidade: {weekly_data['sum'].min():,.0f})\")\n",
    "    print(f\"Coeficiente de varia√ß√£o semanal: {(weekly_data['sum'].std() / weekly_data['sum'].mean()):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. An√°lise por Categoria de Produto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar colunas de categoria de produto\n",
    "print(\"=== AN√ÅLISE POR CATEGORIA DE PRODUTO ===\")\n",
    "\n",
    "# Procurar colunas relacionadas a produtos\n",
    "product_cols = [col for col in main_df.columns if any(keyword in col.lower() \n",
    "               for keyword in ['categoria', 'category', 'produto', 'product', 'item'])]\n",
    "\n",
    "print(f\"Colunas relacionadas a produtos encontradas: {product_cols}\")\n",
    "\n",
    "# Analisar cada coluna de produto\n",
    "for col in product_cols:\n",
    "    if main_df[col].dtype == 'object' or main_df[col].nunique() < 100:\n",
    "        print(f\"\\n--- An√°lise da coluna: {col} ---\")\n",
    "        print(f\"Valores √∫nicos: {main_df[col].nunique()}\")\n",
    "        \n",
    "        value_counts = main_df[col].value_counts()\n",
    "        print(f\"\\nTop 10 categorias:\")\n",
    "        display(value_counts.head(10))\n",
    "        \n",
    "        # Visualiza√ß√£o se n√£o h√° muitas categorias\n",
    "        if main_df[col].nunique() <= 20:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            value_counts.head(15).plot(kind='bar')\n",
    "            plt.title(f'Distribui√ß√£o de {col}')\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Frequ√™ncia')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de vendas por categoria (se temos quantidade e categoria)\n",
    "if qty_col and len(product_cols) > 0:\n",
    "    print(\"=== AN√ÅLISE DE VENDAS POR CATEGORIA ===\")\n",
    "    \n",
    "    # Usar a primeira coluna categ√≥rica encontrada\n",
    "    category_col = None\n",
    "    for col in product_cols:\n",
    "        if main_df[col].dtype == 'object' and main_df[col].nunique() < 50:\n",
    "            category_col = col\n",
    "            break\n",
    "    \n",
    "    if category_col:\n",
    "        # Vendas por categoria\n",
    "        category_sales = main_df.groupby(category_col)[qty_col].agg([\n",
    "            'sum', 'mean', 'median', 'std', 'count'\n",
    "        ]).round(2)\n",
    "        \n",
    "        category_sales = category_sales.sort_values('sum', ascending=False)\n",
    "        \n",
    "        print(f\"Vendas por {category_col}:\")\n",
    "        display(category_sales)\n",
    "        \n",
    "        # Visualiza√ß√µes\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Vendas totais por categoria\n",
    "        top_categories = category_sales.head(10)\n",
    "        axes[0,0].bar(range(len(top_categories)), top_categories['sum'])\n",
    "        axes[0,0].set_title(f'Top 10 Categorias - Vendas Totais')\n",
    "        axes[0,0].set_ylabel('Quantidade Total')\n",
    "        axes[0,0].set_xticks(range(len(top_categories)))\n",
    "        axes[0,0].set_xticklabels(top_categories.index, rotation=45, ha='right')\n",
    "        \n",
    "        # Vendas m√©dias por categoria\n",
    "        axes[0,1].bar(range(len(top_categories)), top_categories['mean'])\n",
    "        axes[0,1].set_title(f'Top 10 Categorias - Vendas M√©dias')\n",
    "        axes[0,1].set_ylabel('Quantidade M√©dia')\n",
    "        axes[0,1].set_xticks(range(len(top_categories)))\n",
    "        axes[0,1].set_xticklabels(top_categories.index, rotation=45, ha='right')\n",
    "        \n",
    "        # N√∫mero de transa√ß√µes por categoria\n",
    "        axes[1,0].bar(range(len(top_categories)), top_categories['count'])\n",
    "        axes[1,0].set_title(f'Top 10 Categorias - N√∫mero de Transa√ß√µes')\n",
    "        axes[1,0].set_ylabel('N√∫mero de Transa√ß√µes')\n",
    "        axes[1,0].set_xticks(range(len(top_categories)))\n",
    "        axes[1,0].set_xticklabels(top_categories.index, rotation=45, ha='right')\n",
    "        \n",
    "        # Boxplot de distribui√ß√£o por categoria (top 8)\n",
    "        top_8_categories = category_sales.head(8).index\n",
    "        data_for_box = [main_df[main_df[category_col] == cat][qty_col].values \n",
    "                       for cat in top_8_categories]\n",
    "        \n",
    "        axes[1,1].boxplot(data_for_box, labels=top_8_categories)\n",
    "        axes[1,1].set_title(f'Distribui√ß√£o de Vendas por Categoria (Top 8)')\n",
    "        axes[1,1].set_ylabel('Quantidade')\n",
    "        axes[1,1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Nenhuma coluna categ√≥rica adequada encontrada para an√°lise de vendas.\")"
   ]
  }  }
,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. An√°lise por Tipo de PDV (Ponto de Venda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar colunas relacionadas a PDV\n",
    "print(\"=== AN√ÅLISE POR TIPO DE PDV ===\")\n",
    "\n",
    "# Procurar colunas relacionadas a PDV/loja\n",
    "store_cols = [col for col in main_df.columns if any(keyword in col.lower() \n",
    "             for keyword in ['pdv', 'store', 'loja', 'premise', 'tipo'])]\n",
    "\n",
    "print(f\"Colunas relacionadas a PDV encontradas: {store_cols}\")\n",
    "\n",
    "# Analisar cada coluna de PDV\n",
    "for col in store_cols:\n",
    "    if main_df[col].dtype == 'object' or main_df[col].nunique() < 100:\n",
    "        print(f\"\\n--- An√°lise da coluna: {col} ---\")\n",
    "        print(f\"Valores √∫nicos: {main_df[col].nunique()}\")\n",
    "        \n",
    "        value_counts = main_df[col].value_counts()\n",
    "        print(f\"\\nDistribui√ß√£o:\")\n",
    "        display(value_counts)\n",
    "        \n",
    "        # Visualiza√ß√£o se n√£o h√° muitos tipos\n",
    "        if main_df[col].nunique() <= 15:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            value_counts.plot(kind='bar')\n",
    "            plt.title(f'Distribui√ß√£o de {col}')\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Frequ√™ncia')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de vendas por tipo de PDV\n",
    "if qty_col and len(store_cols) > 0:\n",
    "    print(\"=== AN√ÅLISE DE VENDAS POR TIPO DE PDV ===\")\n",
    "    \n",
    "    # Usar a primeira coluna de tipo de PDV encontrada\n",
    "    store_type_col = None\n",
    "    for col in store_cols:\n",
    "        if main_df[col].dtype == 'object' and main_df[col].nunique() < 20:\n",
    "            store_type_col = col\n",
    "            break\n",
    "    \n",
    "    if store_type_col:\n",
    "        # Vendas por tipo de PDV\n",
    "        store_sales = main_df.groupby(store_type_col)[qty_col].agg([\n",
    "            'sum', 'mean', 'median', 'std', 'count'\n",
    "        ]).round(2)\n",
    "        \n",
    "        store_sales = store_sales.sort_values('sum', ascending=False)\n",
    "        \n",
    "        print(f\"Vendas por {store_type_col}:\")\n",
    "        display(store_sales)\n",
    "        \n",
    "        # Visualiza√ß√µes\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Vendas totais por tipo de PDV\n",
    "        axes[0,0].bar(store_sales.index, store_sales['sum'])\n",
    "        axes[0,0].set_title(f'Vendas Totais por {store_type_col}')\n",
    "        axes[0,0].set_ylabel('Quantidade Total')\n",
    "        axes[0,0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Vendas m√©dias por tipo de PDV\n",
    "        axes[0,1].bar(store_sales.index, store_sales['mean'])\n",
    "        axes[0,1].set_title(f'Vendas M√©dias por {store_type_col}')\n",
    "        axes[0,1].set_ylabel('Quantidade M√©dia')\n",
    "        axes[0,1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # N√∫mero de transa√ß√µes por tipo de PDV\n",
    "        axes[1,0].bar(store_sales.index, store_sales['count'])\n",
    "        axes[1,0].set_title(f'N√∫mero de Transa√ß√µes por {store_type_col}')\n",
    "        axes[1,0].set_ylabel('N√∫mero de Transa√ß√µes')\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Boxplot de distribui√ß√£o por tipo de PDV\n",
    "        data_for_box = [main_df[main_df[store_type_col] == store_type][qty_col].values \n",
    "                       for store_type in store_sales.index]\n",
    "        \n",
    "        axes[1,1].boxplot(data_for_box, labels=store_sales.index)\n",
    "        axes[1,1].set_title(f'Distribui√ß√£o de Vendas por {store_type_col}')\n",
    "        axes[1,1].set_ylabel('Quantidade')\n",
    "        axes[1,1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # An√°lise de performance por tipo de PDV\n",
    "        print(f\"\\n=== PERFORMANCE POR TIPO DE PDV ===\")\n",
    "        performance_metrics = pd.DataFrame({\n",
    "            'Tipo_PDV': store_sales.index,\n",
    "            'Vendas_Totais': store_sales['sum'].values,\n",
    "            'Vendas_Medias': store_sales['mean'].values,\n",
    "            'Num_Transacoes': store_sales['count'].values,\n",
    "            'Desvio_Padrao': store_sales['std'].values\n",
    "        })\n",
    "        \n",
    "        # Calcular coeficiente de varia√ß√£o\n",
    "        performance_metrics['Coef_Variacao'] = (performance_metrics['Desvio_Padrao'] / \n",
    "                                              performance_metrics['Vendas_Medias']).round(3)\n",
    "        \n",
    "        display(performance_metrics)\n",
    "    else:\n",
    "        print(\"Nenhuma coluna de tipo de PDV adequada encontrada para an√°lise de vendas.\")"
   ]
  }  },
 
 {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Identifica√ß√£o de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de outliers em colunas num√©ricas\n",
    "print(\"=== IDENTIFICA√á√ÉO DE OUTLIERS ===\")\n",
    "\n",
    "numeric_columns = main_df.select_dtypes(include=[np.number]).columns\n",
    "print(f\"Colunas num√©ricas para an√°lise: {list(numeric_columns)}\")\n",
    "\n",
    "outlier_summary = []\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if main_df[col].nunique() > 10:  # Evitar colunas com poucos valores √∫nicos\n",
    "        print(f\"\\n--- An√°lise de outliers: {col} ---\")\n",
    "        \n",
    "        # Estat√≠sticas b√°sicas\n",
    "        Q1 = main_df[col].quantile(0.25)\n",
    "        Q3 = main_df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Limites para outliers\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Identificar outliers\n",
    "        outliers_low = main_df[main_df[col] < lower_bound]\n",
    "        outliers_high = main_df[main_df[col] > upper_bound]\n",
    "        total_outliers = len(outliers_low) + len(outliers_high)\n",
    "        \n",
    "        outlier_percentage = (total_outliers / len(main_df)) * 100\n",
    "        \n",
    "        print(f\"Q1: {Q1:.2f}, Q3: {Q3:.2f}, IQR: {IQR:.2f}\")\n",
    "        print(f\"Limites: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "        print(f\"Outliers baixos: {len(outliers_low)} ({len(outliers_low)/len(main_df)*100:.2f}%)\")\n",
    "        print(f\"Outliers altos: {len(outliers_high)} ({len(outliers_high)/len(main_df)*100:.2f}%)\")\n",
    "        print(f\"Total de outliers: {total_outliers} ({outlier_percentage:.2f}%)\")\n",
    "        \n",
    "        # Armazenar resumo\n",
    "        outlier_summary.append({\n",
    "            'Coluna': col,\n",
    "            'Total_Outliers': total_outliers,\n",
    "            'Percentual': outlier_percentage,\n",
    "            'Outliers_Baixos': len(outliers_low),\n",
    "            'Outliers_Altos': len(outliers_high),\n",
    "            'Q1': Q1,\n",
    "            'Q3': Q3,\n",
    "            'IQR': IQR\n",
    "        })\n",
    "\n",
    "# Resumo de outliers\n",
    "if outlier_summary:\n",
    "    outlier_df = pd.DataFrame(outlier_summary)\n",
    "    outlier_df = outlier_df.sort_values('Percentual', ascending=False)\n",
    "    \n",
    "    print(\"\\n=== RESUMO DE OUTLIERS ===\")\n",
    "    display(outlier_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o de outliers\n",
    "if len(numeric_columns) > 0:\n",
    "    print(\"=== VISUALIZA√á√ÉO DE OUTLIERS ===\")\n",
    "    \n",
    "    # Selecionar at√© 4 colunas num√©ricas mais relevantes\n",
    "    cols_to_plot = list(numeric_columns)[:4]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, len(cols_to_plot), figsize=(5*len(cols_to_plot), 10))\n",
    "    \n",
    "    if len(cols_to_plot) == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    for i, col in enumerate(cols_to_plot):\n",
    "        # Boxplot\n",
    "        axes[0, i].boxplot(main_df[col].dropna())\n",
    "        axes[0, i].set_title(f'Boxplot - {col}')\n",
    "        axes[0, i].set_ylabel('Valores')\n",
    "        \n",
    "        # Histograma\n",
    "        axes[1, i].hist(main_df[col].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "        axes[1, i].set_title(f'Histograma - {col}')\n",
    "        axes[1, i].set_xlabel('Valores')\n",
    "        axes[1, i].set_ylabel('Frequ√™ncia')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # An√°lise espec√≠fica da coluna de quantidade (se existir)\n",
    "    if qty_col and qty_col in main_df.columns:\n",
    "        print(f\"\\n=== AN√ÅLISE DETALHADA DE OUTLIERS - {qty_col} ===\")\n",
    "        \n",
    "        qty_data = main_df[qty_col].dropna()\n",
    "        \n",
    "        # Estat√≠sticas detalhadas\n",
    "        print(f\"Estat√≠sticas de {qty_col}:\")\n",
    "        print(f\"M√≠nimo: {qty_data.min()}\")\n",
    "        print(f\"M√°ximo: {qty_data.max()}\")\n",
    "        print(f\"M√©dia: {qty_data.mean():.2f}\")\n",
    "        print(f\"Mediana: {qty_data.median():.2f}\")\n",
    "        print(f\"Desvio padr√£o: {qty_data.std():.2f}\")\n",
    "        \n",
    "        # Percentis\n",
    "        percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]\n",
    "        print(f\"\\nPercentis:\")\n",
    "        for p in percentiles:\n",
    "            value = qty_data.quantile(p/100)\n",
    "            print(f\"P{p}: {value:.2f}\")\n",
    "        \n",
    "        # Visualiza√ß√£o detalhada\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        \n",
    "        # Histograma completo\n",
    "        axes[0].hist(qty_data, bins=100, alpha=0.7, edgecolor='black')\n",
    "        axes[0].set_title(f'Distribui√ß√£o Completa - {qty_col}')\n",
    "        axes[0].set_xlabel('Quantidade')\n",
    "        axes[0].set_ylabel('Frequ√™ncia')\n",
    "        \n",
    "        # Histograma sem outliers extremos (99% dos dados)\n",
    "        p99 = qty_data.quantile(0.99)\n",
    "        qty_filtered = qty_data[qty_data <= p99]\n",
    "        axes[1].hist(qty_filtered, bins=50, alpha=0.7, edgecolor='black', color='orange')\n",
    "        axes[1].set_title(f'Distribui√ß√£o (at√© P99) - {qty_col}')\n",
    "        axes[1].set_xlabel('Quantidade')\n",
    "        axes[1].set_ylabel('Frequ√™ncia')\n",
    "        \n",
    "        # Boxplot detalhado\n",
    "        axes[2].boxplot(qty_data, patch_artist=True)\n",
    "        axes[2].set_title(f'Boxplot Detalhado - {qty_col}')\n",
    "        axes[2].set_ylabel('Quantidade')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  }  },
 
 {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. An√°lise de Correla√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de correla√ß√µes entre vari√°veis num√©ricas\n",
    "print(\"=== AN√ÅLISE DE CORRELA√á√ïES ===\")\n",
    "\n",
    "# Selecionar apenas colunas num√©ricas com variabilidade\n",
    "numeric_cols_for_corr = []\n",
    "for col in numeric_columns:\n",
    "    if main_df[col].nunique() > 1 and main_df[col].std() > 0:\n",
    "        numeric_cols_for_corr.append(col)\n",
    "\n",
    "print(f\"Colunas num√©ricas para an√°lise de correla√ß√£o: {numeric_cols_for_corr}\")\n",
    "\n",
    "if len(numeric_cols_for_corr) > 1:\n",
    "    # Calcular matriz de correla√ß√£o\n",
    "    correlation_matrix = main_df[numeric_cols_for_corr].corr()\n",
    "    \n",
    "    print(\"\\nMatriz de correla√ß√£o:\")\n",
    "    display(correlation_matrix.round(3))\n",
    "    \n",
    "    # Visualiza√ß√£o da matriz de correla√ß√£o\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "    plt.title('Matriz de Correla√ß√£o - Vari√°veis Num√©ricas')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identificar correla√ß√µes mais fortes\n",
    "    print(\"\\n=== CORRELA√á√ïES MAIS FORTES ===\")\n",
    "    \n",
    "    # Extrair correla√ß√µes (excluindo diagonal)\n",
    "    correlations = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            var1 = correlation_matrix.columns[i]\n",
    "            var2 = correlation_matrix.columns[j]\n",
    "            corr_value = correlation_matrix.iloc[i, j]\n",
    "            correlations.append({\n",
    "                'Variavel_1': var1,\n",
    "                'Variavel_2': var2,\n",
    "                'Correlacao': corr_value,\n",
    "                'Correlacao_Abs': abs(corr_value)\n",
    "            })\n",
    "    \n",
    "    corr_df = pd.DataFrame(correlations)\n",
    "    corr_df = corr_df.sort_values('Correlacao_Abs', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 correla√ß√µes mais fortes:\")\n",
    "    display(corr_df.head(10)[['Variavel_1', 'Variavel_2', 'Correlacao']])\n",
    "    \n",
    "    # Identificar correla√ß√µes problem√°ticas (multicolinearidade)\n",
    "    high_corr = corr_df[corr_df['Correlacao_Abs'] > 0.8]\n",
    "    if len(high_corr) > 0:\n",
    "        print(\"\\n‚ö†Ô∏è ATEN√á√ÉO: Correla√ß√µes muito altas detectadas (>0.8):\")\n",
    "        display(high_corr[['Variavel_1', 'Variavel_2', 'Correlacao']])\n",
    "        print(\"Considere remover uma das vari√°veis para evitar multicolinearidade.\")\n",
    "else:\n",
    "    print(\"N√£o h√° colunas num√©ricas suficientes para an√°lise de correla√ß√£o.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Insights e Recomenda√ß√µes para Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo dos insights encontrados\n",
    "print(\"=== RESUMO DOS INSIGHTS PARA FEATURE ENGINEERING ===\")\n",
    "\n",
    "insights = []\n",
    "\n",
    "# Insights sobre dados temporais\n",
    "if date_col:\n",
    "    insights.append(\"‚úÖ TEMPORAL: Dados temporais identificados - criar features de sazonalidade\")\n",
    "    if 'semana' in df_temporal.columns:\n",
    "        cv_semanal = df_temporal.groupby('semana')[qty_col].sum().std() / df_temporal.groupby('semana')[qty_col].sum().mean()\n",
    "        if cv_semanal > 0.3:\n",
    "            insights.append(f\"üìà SAZONALIDADE: Alta variabilidade semanal (CV={cv_semanal:.3f}) - importante para o modelo\")\n",
    "        else:\n",
    "            insights.append(f\"üìä SAZONALIDADE: Variabilidade semanal moderada (CV={cv_semanal:.3f})\")\n",
    "else:\n",
    "    insights.append(\"‚ùå TEMPORAL: Nenhuma coluna de data identificada - verificar dados\")\n",
    "\n",
    "# Insights sobre categorias de produto\n",
    "if len(product_cols) > 0:\n",
    "    insights.append(f\"‚úÖ PRODUTOS: {len(product_cols)} colunas de produto identificadas - usar para encoding\")\n",
    "    for col in product_cols[:2]:  # Primeiras 2 colunas\n",
    "        unique_count = main_df[col].nunique()\n",
    "        if unique_count < 50:\n",
    "            insights.append(f\"üè∑Ô∏è CATEGORIA: {col} tem {unique_count} categorias - adequado para one-hot encoding\")\n",
    "        else:\n",
    "            insights.append(f\"üè∑Ô∏è CATEGORIA: {col} tem {unique_count} categorias - considerar target encoding\")\n",
    "else:\n",
    "    insights.append(\"‚ùå PRODUTOS: Nenhuma coluna de categoria identificada\")\n",
    "\n",
    "# Insights sobre PDVs\n",
    "if len(store_cols) > 0:\n",
    "    insights.append(f\"‚úÖ PDV: {len(store_cols)} colunas de PDV identificadas\")\n",
    "    for col in store_cols[:2]:  # Primeiras 2 colunas\n",
    "        unique_count = main_df[col].nunique()\n",
    "        if unique_count < 20:\n",
    "            insights.append(f\"üè™ TIPO_PDV: {col} tem {unique_count} tipos - criar features por tipo\")\n",
    "else:\n",
    "    insights.append(\"‚ùå PDV: Nenhuma coluna de PDV identificada\")\n",
    "\n",
    "# Insights sobre outliers\n",
    "if qty_col and outlier_summary:\n",
    "    qty_outliers = next((item for item in outlier_summary if item['Coluna'] == qty_col), None)\n",
    "    if qty_outliers and qty_outliers['Percentual'] > 5:\n",
    "        insights.append(f\"‚ö†Ô∏è OUTLIERS: {qty_outliers['Percentual']:.1f}% outliers em quantidade - aplicar winsoriza√ß√£o\")\n",
    "    elif qty_outliers:\n",
    "        insights.append(f\"‚úÖ OUTLIERS: {qty_outliers['Percentual']:.1f}% outliers em quantidade - n√≠vel aceit√°vel\")\n",
    "\n",
    "# Insights sobre dados faltantes\n",
    "total_missing = main_df.isnull().sum().sum()\n",
    "if total_missing > 0:\n",
    "    missing_percent = (total_missing / (len(main_df) * len(main_df.columns))) * 100\n",
    "    if missing_percent > 10:\n",
    "        insights.append(f\"‚ö†Ô∏è MISSING: {missing_percent:.1f}% dados faltantes - estrat√©gia de imputa√ß√£o necess√°ria\")\n",
    "    else:\n",
    "        insights.append(f\"‚úÖ MISSING: {missing_percent:.1f}% dados faltantes - n√≠vel baixo\")\n",
    "else:\n",
    "    insights.append(\"‚úÖ MISSING: Nenhum dado faltante encontrado\")\n",
    "\n",
    "# Insights sobre correla√ß√µes\n",
    "if len(numeric_cols_for_corr) > 1:\n",
    "    high_corr_count = len(corr_df[corr_df['Correlacao_Abs'] > 0.8])\n",
    "    if high_corr_count > 0:\n",
    "        insights.append(f\"‚ö†Ô∏è CORRELA√á√ÉO: {high_corr_count} pares com correla√ß√£o >0.8 - risco de multicolinearidade\")\n",
    "    else:\n",
    "        insights.append(\"‚úÖ CORRELA√á√ÉO: Sem correla√ß√µes problem√°ticas detectadas\")\n",
    "\n",
    "# Exibir insights\n",
    "for insight in insights:\n",
    "    print(insight)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMENDA√á√ïES PARA PR√ìXIMAS ETAPAS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "recommendations = [\n",
    "    \"1. üïí Criar features temporais: semana, m√™s, trimestre, sazonalidade\",\n",
    "    \"2. üìä Implementar features de lag: vendas das √∫ltimas 1, 2, 4, 8 semanas\",\n",
    "    \"3. üìà Calcular m√©dias m√≥veis e estat√≠sticas rolling por PDV/produto\",\n",
    "    \"4. üè∑Ô∏è Aplicar encoding adequado para vari√°veis categ√≥ricas\",\n",
    "    \"5. üéØ Criar features de performance hist√≥rica por PDV e categoria\",\n",
    "    \"6. üßπ Implementar tratamento de outliers (winsoriza√ß√£o ou capping)\",\n",
    "    \"7. üîÑ Desenvolver estrat√©gia de valida√ß√£o temporal (walk-forward)\",\n",
    "    \"8. üìã Criar features de ranking e percentis por categoria\"\n",
    "]\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sum√°rio Executivo da EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar sum√°rio executivo\n",
    "print(\"=== SUM√ÅRIO EXECUTIVO DA AN√ÅLISE EXPLORAT√ìRIA ===\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Informa√ß√µes b√°sicas do dataset\n",
    "print(f\"üìä DATASET PRINCIPAL:\")\n",
    "print(f\"   ‚Ä¢ Registros: {len(main_df):,}\")\n",
    "print(f\"   ‚Ä¢ Colunas: {len(main_df.columns)}\")\n",
    "print(f\"   ‚Ä¢ Mem√≥ria: {main_df.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Qualidade dos dados\n",
    "missing_total = main_df.isnull().sum().sum()\n",
    "duplicates_total = main_df.duplicated().sum()\n",
    "print(f\"\\nüîç QUALIDADE DOS DADOS:\")\n",
    "print(f\"   ‚Ä¢ Valores faltantes: {missing_total:,} ({missing_total/(len(main_df)*len(main_df.columns))*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Registros duplicados: {duplicates_total:,} ({duplicates_total/len(main_df)*100:.1f}%)\")\n",
    "\n",
    "# Informa√ß√µes temporais\n",
    "if date_col:\n",
    "    date_range = f\"{df_temporal[date_col].min().strftime('%Y-%m-%d')} a {df_temporal[date_col].max().strftime('%Y-%m-%d')}\"\n",
    "    weeks_count = df_temporal['semana'].nunique() if 'semana' in df_temporal.columns else 'N/A'\n",
    "    print(f\"\\nüìÖ INFORMA√á√ïES TEMPORAIS:\")\n",
    "    print(f\"   ‚Ä¢ Per√≠odo: {date_range}\")\n",
    "    print(f\"   ‚Ä¢ Semanas √∫nicas: {weeks_count}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå INFORMA√á√ïES TEMPORAIS: N√£o identificadas\")\n",
    "\n",
    "# Informa√ß√µes de vendas\n",
    "if qty_col:\n",
    "    total_sales = main_df[qty_col].sum()\n",
    "    avg_sales = main_df[qty_col].mean()\n",
    "    print(f\"\\nüí∞ INFORMA√á√ïES DE VENDAS:\")\n",
    "    print(f\"   ‚Ä¢ Total vendido: {total_sales:,.0f} unidades\")\n",
    "    print(f\"   ‚Ä¢ M√©dia por transa√ß√£o: {avg_sales:.1f} unidades\")\n",
    "    print(f\"   ‚Ä¢ Transa√ß√µes: {len(main_df):,}\")\n",
    "\n",
    "# Diversidade de produtos e PDVs\n",
    "if product_cols:\n",
    "    main_product_col = product_cols[0]\n",
    "    product_count = main_df[main_product_col].nunique()\n",
    "    print(f\"\\nüè∑Ô∏è DIVERSIDADE DE PRODUTOS:\")\n",
    "    print(f\"   ‚Ä¢ Produtos √∫nicos: {product_count:,}\")\n",
    "\n",
    "if store_cols:\n",
    "    main_store_col = store_cols[0]\n",
    "    store_count = main_df[main_store_col].nunique()\n",
    "    print(f\"\\nüè™ DIVERSIDADE DE PDVs:\")\n",
    "    print(f\"   ‚Ä¢ PDVs √∫nicos: {store_count:,}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ AN√ÅLISE EXPLORAT√ìRIA CONCLU√çDA\")\n",
    "print(f\"üìã Pr√≥ximo passo: Feature Engineering (Task 5)\")\n",
    "print(f\"=\"*60)"
   ]
  }  }
 ],

 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}