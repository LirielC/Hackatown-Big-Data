{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise Exploratória de Dados (EDA) - Hackathon Forecast 2025\n",
    "\n",
    "Este notebook contém a análise exploratória completa dos dados de vendas para o modelo de previsão.\n",
    "\n",
    "## Objetivos:\n",
    "- Entender a estrutura e qualidade dos dados\n",
    "- Identificar padrões temporais e sazonalidade\n",
    "- Analisar distribuições por categoria de produto e tipo de PDV\n",
    "- Identificar outliers e dados faltantes\n",
    "- Gerar insights para feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necessários\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Adicionar src ao path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data.ingestion import DataIngestion\n",
    "from data.preprocessing import DataPreprocessor\n",
    "\n",
    "# Configurações\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Configurar tamanho das figuras\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}  }
,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento e Primeira Inspeção dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar classes de ingestão e preprocessamento\n",
    "ingestion = DataIngestion(use_polars=True)\n",
    "preprocessor = DataPreprocessor(use_polars=False)\n",
    "\n",
    "# Carregar dados dos arquivos Parquet\n",
    "data_path = Path('../hackathon_2025_templates')\n",
    "print(f\"Carregando dados de: {data_path}\")\n",
    "\n",
    "# Carregar múltiplos schemas se necessário\n",
    "try:\n",
    "    data_schemas = ingestion.load_multiple_schemas(data_path, sample_only=True)\n",
    "    print(f\"\\nEncontrados {len(data_schemas)} schemas diferentes:\")\n",
    "    \n",
    "    for i, (schema, df) in enumerate(data_schemas.items()):\n",
    "        print(f\"\\nSchema {i+1}: {len(df)} registros\")\n",
    "        print(f\"Colunas: {list(schema)}\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        \n",
    "        # Mostrar primeiras linhas\n",
    "        display(df.head())\n",
    "        \n",
    "        # Informações básicas\n",
    "        print(\"\\nInfo básica:\")\n",
    "        print(df.info())\n",
    "        \n",
    "        # Salvar o maior dataset para análise principal\n",
    "        if i == 0 or len(df) > len(main_df):\n",
    "            main_df = df.copy()\n",
    "            main_schema = schema\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar dados: {e}\")\n",
    "    # Fallback: tentar carregar arquivo por arquivo\n",
    "    parquet_files = list(data_path.glob(\"*.parquet\"))\n",
    "    main_df = pd.read_parquet(parquet_files[0])\n",
    "    print(f\"Carregado arquivo individual: {parquet_files[0].name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise inicial do dataset principal\n",
    "print(\"=== ANÁLISE INICIAL DO DATASET PRINCIPAL ===\")\n",
    "print(f\"Shape: {main_df.shape}\")\n",
    "print(f\"Colunas: {list(main_df.columns)}\")\n",
    "print(f\"Tipos de dados:\")\n",
    "print(main_df.dtypes)\n",
    "\n",
    "print(\"\\n=== PRIMEIRAS 10 LINHAS ===\")\n",
    "display(main_df.head(10))\n",
    "\n",
    "print(\"\\n=== ÚLTIMAS 5 LINHAS ===\")\n",
    "display(main_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estatísticas descritivas\n",
    "print(\"=== ESTATÍSTICAS DESCRITIVAS ===\")\n",
    "\n",
    "# Para colunas numéricas\n",
    "numeric_cols = main_df.select_dtypes(include=[np.number]).columns\n",
    "if len(numeric_cols) > 0:\n",
    "    print(\"\\nColunas numéricas:\")\n",
    "    display(main_df[numeric_cols].describe())\n",
    "\n",
    "# Para colunas categóricas\n",
    "categorical_cols = main_df.select_dtypes(include=['object']).columns\n",
    "if len(categorical_cols) > 0:\n",
    "    print(\"\\nColunas categóricas:\")\n",
    "    for col in categorical_cols:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Valores únicos: {main_df[col].nunique()}\")\n",
    "        print(f\"  Top 5 valores:\")\n",
    "        print(main_df[col].value_counts().head())"
   ]
  }  
},
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Análise de Qualidade dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de valores faltantes\n",
    "print(\"=== ANÁLISE DE VALORES FALTANTES ===\")\n",
    "\n",
    "missing_data = main_df.isnull().sum()\n",
    "missing_percent = (missing_data / len(main_df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Coluna': missing_data.index,\n",
    "    'Valores_Faltantes': missing_data.values,\n",
    "    'Percentual': missing_percent.values\n",
    "})\n",
    "\n",
    "missing_df = missing_df[missing_df['Valores_Faltantes'] > 0].sort_values('Percentual', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    display(missing_df)\n",
    "    \n",
    "    # Visualização de valores faltantes\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=missing_df, x='Coluna', y='Percentual')\n",
    "    plt.title('Percentual de Valores Faltantes por Coluna')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Percentual (%)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Nenhum valor faltante encontrado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de duplicatas\n",
    "print(\"=== ANÁLISE DE DUPLICATAS ===\")\n",
    "\n",
    "total_duplicates = main_df.duplicated().sum()\n",
    "duplicate_percent = (total_duplicates / len(main_df)) * 100\n",
    "\n",
    "print(f\"Total de registros duplicados: {total_duplicates} ({duplicate_percent:.2f}%)\")\n",
    "\n",
    "if total_duplicates > 0:\n",
    "    print(\"\\nPrimeiros registros duplicados:\")\n",
    "    duplicated_rows = main_df[main_df.duplicated(keep=False)]\n",
    "    display(duplicated_rows.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de tipos de dados e conversões necessárias\n",
    "print(\"=== ANÁLISE DE TIPOS DE DADOS ===\")\n",
    "\n",
    "# Identificar possíveis colunas de data\n",
    "date_columns = [col for col in main_df.columns if any(keyword in col.lower() \n",
    "                for keyword in ['date', 'data', 'time', 'timestamp'])]\n",
    "\n",
    "print(f\"Possíveis colunas de data: {date_columns}\")\n",
    "\n",
    "# Identificar possíveis colunas de ID\n",
    "id_columns = [col for col in main_df.columns if any(keyword in col.lower() \n",
    "              for keyword in ['id', 'pdv', 'produto', 'store', 'product'])]\n",
    "\n",
    "print(f\"Possíveis colunas de ID: {id_columns}\")\n",
    "\n",
    "# Identificar possíveis colunas de quantidade/valor\n",
    "quantity_columns = [col for col in main_df.columns if any(keyword in col.lower() \n",
    "                   for keyword in ['quantidade', 'quantity', 'value', 'valor', 'sales'])]\n",
    "\n",
    "print(f\"Possíveis colunas de quantidade/valor: {quantity_columns}\")\n",
    "\n",
    "# Mostrar estatísticas de memória\n",
    "memory_usage = main_df.memory_usage(deep=True)\n",
    "total_memory = memory_usage.sum() / 1024 / 1024  # MB\n",
    "print(f\"\\nUso total de memória: {total_memory:.2f} MB\")"
   ]
  }  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Análise Temporal e Sazonalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados para análise temporal\n",
    "print(\"=== PREPARAÇÃO PARA ANÁLISE TEMPORAL ===\")\n",
    "\n",
    "# Tentar identificar e converter coluna de data\n",
    "df_temporal = main_df.copy()\n",
    "\n",
    "# Procurar coluna de data\n",
    "date_col = None\n",
    "for col in df_temporal.columns:\n",
    "    if any(keyword in col.lower() for keyword in ['date', 'data', 'time']):\n",
    "        try:\n",
    "            df_temporal[col] = pd.to_datetime(df_temporal[col])\n",
    "            date_col = col\n",
    "            print(f\"Coluna de data identificada: {col}\")\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "if date_col:\n",
    "    # Filtrar apenas dados de 2022\n",
    "    df_temporal = df_temporal[df_temporal[date_col].dt.year == 2022]\n",
    "    print(f\"Dados filtrados para 2022: {len(df_temporal)} registros\")\n",
    "    \n",
    "    # Criar features temporais\n",
    "    df_temporal['ano'] = df_temporal[date_col].dt.year\n",
    "    df_temporal['mes'] = df_temporal[date_col].dt.month\n",
    "    df_temporal['semana'] = df_temporal[date_col].dt.isocalendar().week\n",
    "    df_temporal['dia_semana'] = df_temporal[date_col].dt.dayofweek\n",
    "    df_temporal['trimestre'] = df_temporal[date_col].dt.quarter\n",
    "    \n",
    "    print(f\"Range de datas: {df_temporal[date_col].min()} a {df_temporal[date_col].max()}\")\n",
    "    print(f\"Total de semanas: {df_temporal['semana'].nunique()}\")\n",
    "else:\n",
    "    print(\"AVISO: Nenhuma coluna de data identificada!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de distribuição temporal\n",
    "if date_col:\n",
    "    print(\"=== ANÁLISE DE DISTRIBUIÇÃO TEMPORAL ===\")\n",
    "    \n",
    "    # Identificar coluna de quantidade\n",
    "    qty_col = None\n",
    "    for col in df_temporal.columns:\n",
    "        if any(keyword in col.lower() for keyword in ['quantidade', 'quantity', 'sales']):\n",
    "            if df_temporal[col].dtype in ['int64', 'float64']:\n",
    "                qty_col = col\n",
    "                break\n",
    "    \n",
    "    if qty_col:\n",
    "        # Agregação por mês\n",
    "        monthly_sales = df_temporal.groupby('mes')[qty_col].agg(['sum', 'mean', 'count']).reset_index()\n",
    "        monthly_sales['mes_nome'] = monthly_sales['mes'].map({\n",
    "            1: 'Jan', 2: 'Fev', 3: 'Mar', 4: 'Abr', 5: 'Mai', 6: 'Jun',\n",
    "            7: 'Jul', 8: 'Ago', 9: 'Set', 10: 'Out', 11: 'Nov', 12: 'Dez'\n",
    "        })\n",
    "        \n",
    "        print(\"Vendas por mês:\")\n",
    "        display(monthly_sales)\n",
    "        \n",
    "        # Visualização mensal\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Vendas totais por mês\n",
    "        axes[0,0].bar(monthly_sales['mes_nome'], monthly_sales['sum'])\n",
    "        axes[0,0].set_title('Vendas Totais por Mês')\n",
    "        axes[0,0].set_ylabel('Quantidade Total')\n",
    "        axes[0,0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Vendas médias por mês\n",
    "        axes[0,1].bar(monthly_sales['mes_nome'], monthly_sales['mean'])\n",
    "        axes[0,1].set_title('Vendas Médias por Mês')\n",
    "        axes[0,1].set_ylabel('Quantidade Média')\n",
    "        axes[0,1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Número de transações por mês\n",
    "        axes[1,0].bar(monthly_sales['mes_nome'], monthly_sales['count'])\n",
    "        axes[1,0].set_title('Número de Transações por Mês')\n",
    "        axes[1,0].set_ylabel('Número de Transações')\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Vendas por dia da semana\n",
    "        weekly_sales = df_temporal.groupby('dia_semana')[qty_col].sum()\n",
    "        dias_semana = ['Seg', 'Ter', 'Qua', 'Qui', 'Sex', 'Sáb', 'Dom']\n",
    "        axes[1,1].bar(dias_semana, weekly_sales.values)\n",
    "        axes[1,1].set_title('Vendas por Dia da Semana')\n",
    "        axes[1,1].set_ylabel('Quantidade Total')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"AVISO: Nenhuma coluna de quantidade identificada!\")"
   ]
  }  }
,
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de sazonalidade semanal\n",
    "if date_col and qty_col:\n",
    "    print(\"=== ANÁLISE DE SAZONALIDADE SEMANAL ===\")\n",
    "    \n",
    "    # Agregação por semana\n",
    "    weekly_data = df_temporal.groupby(['semana'])[qty_col].agg(['sum', 'mean', 'count']).reset_index()\n",
    "    \n",
    "    # Visualização da série temporal semanal\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(weekly_data['semana'], weekly_data['sum'], marker='o', linewidth=2)\n",
    "    plt.title('Evolução das Vendas Semanais em 2022')\n",
    "    plt.xlabel('Semana do Ano')\n",
    "    plt.ylabel('Quantidade Total')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(weekly_data['semana'], weekly_data['mean'], marker='s', color='orange', linewidth=2)\n",
    "    plt.title('Média de Vendas por Transação - Semanal')\n",
    "    plt.xlabel('Semana do Ano')\n",
    "    plt.ylabel('Quantidade Média por Transação')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Estatísticas de sazonalidade\n",
    "    print(f\"\\nEstatísticas semanais:\")\n",
    "    print(f\"Semana com maior venda: {weekly_data.loc[weekly_data['sum'].idxmax(), 'semana']} (Quantidade: {weekly_data['sum'].max():,.0f})\")\n",
    "    print(f\"Semana com menor venda: {weekly_data.loc[weekly_data['sum'].idxmin(), 'semana']} (Quantidade: {weekly_data['sum'].min():,.0f})\")\n",
    "    print(f\"Coeficiente de variação semanal: {(weekly_data['sum'].std() / weekly_data['sum'].mean()):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Análise por Categoria de Produto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar colunas de categoria de produto\n",
    "print(\"=== ANÁLISE POR CATEGORIA DE PRODUTO ===\")\n",
    "\n",
    "# Procurar colunas relacionadas a produtos\n",
    "product_cols = [col for col in main_df.columns if any(keyword in col.lower() \n",
    "               for keyword in ['categoria', 'category', 'produto', 'product', 'item'])]\n",
    "\n",
    "print(f\"Colunas relacionadas a produtos encontradas: {product_cols}\")\n",
    "\n",
    "# Analisar cada coluna de produto\n",
    "for col in product_cols:\n",
    "    if main_df[col].dtype == 'object' or main_df[col].nunique() < 100:\n",
    "        print(f\"\\n--- Análise da coluna: {col} ---\")\n",
    "        print(f\"Valores únicos: {main_df[col].nunique()}\")\n",
    "        \n",
    "        value_counts = main_df[col].value_counts()\n",
    "        print(f\"\\nTop 10 categorias:\")\n",
    "        display(value_counts.head(10))\n",
    "        \n",
    "        # Visualização se não há muitas categorias\n",
    "        if main_df[col].nunique() <= 20:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            value_counts.head(15).plot(kind='bar')\n",
    "            plt.title(f'Distribuição de {col}')\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Frequência')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de vendas por categoria (se temos quantidade e categoria)\n",
    "if qty_col and len(product_cols) > 0:\n",
    "    print(\"=== ANÁLISE DE VENDAS POR CATEGORIA ===\")\n",
    "    \n",
    "    # Usar a primeira coluna categórica encontrada\n",
    "    category_col = None\n",
    "    for col in product_cols:\n",
    "        if main_df[col].dtype == 'object' and main_df[col].nunique() < 50:\n",
    "            category_col = col\n",
    "            break\n",
    "    \n",
    "    if category_col:\n",
    "        # Vendas por categoria\n",
    "        category_sales = main_df.groupby(category_col)[qty_col].agg([\n",
    "            'sum', 'mean', 'median', 'std', 'count'\n",
    "        ]).round(2)\n",
    "        \n",
    "        category_sales = category_sales.sort_values('sum', ascending=False)\n",
    "        \n",
    "        print(f\"Vendas por {category_col}:\")\n",
    "        display(category_sales)\n",
    "        \n",
    "        # Visualizações\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Vendas totais por categoria\n",
    "        top_categories = category_sales.head(10)\n",
    "        axes[0,0].bar(range(len(top_categories)), top_categories['sum'])\n",
    "        axes[0,0].set_title(f'Top 10 Categorias - Vendas Totais')\n",
    "        axes[0,0].set_ylabel('Quantidade Total')\n",
    "        axes[0,0].set_xticks(range(len(top_categories)))\n",
    "        axes[0,0].set_xticklabels(top_categories.index, rotation=45, ha='right')\n",
    "        \n",
    "        # Vendas médias por categoria\n",
    "        axes[0,1].bar(range(len(top_categories)), top_categories['mean'])\n",
    "        axes[0,1].set_title(f'Top 10 Categorias - Vendas Médias')\n",
    "        axes[0,1].set_ylabel('Quantidade Média')\n",
    "        axes[0,1].set_xticks(range(len(top_categories)))\n",
    "        axes[0,1].set_xticklabels(top_categories.index, rotation=45, ha='right')\n",
    "        \n",
    "        # Número de transações por categoria\n",
    "        axes[1,0].bar(range(len(top_categories)), top_categories['count'])\n",
    "        axes[1,0].set_title(f'Top 10 Categorias - Número de Transações')\n",
    "        axes[1,0].set_ylabel('Número de Transações')\n",
    "        axes[1,0].set_xticks(range(len(top_categories)))\n",
    "        axes[1,0].set_xticklabels(top_categories.index, rotation=45, ha='right')\n",
    "        \n",
    "        # Boxplot de distribuição por categoria (top 8)\n",
    "        top_8_categories = category_sales.head(8).index\n",
    "        data_for_box = [main_df[main_df[category_col] == cat][qty_col].values \n",
    "                       for cat in top_8_categories]\n",
    "        \n",
    "        axes[1,1].boxplot(data_for_box, labels=top_8_categories)\n",
    "        axes[1,1].set_title(f'Distribuição de Vendas por Categoria (Top 8)')\n",
    "        axes[1,1].set_ylabel('Quantidade')\n",
    "        axes[1,1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Nenhuma coluna categórica adequada encontrada para análise de vendas.\")"
   ]
  }  }
,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Análise por Tipo de PDV (Ponto de Venda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar colunas relacionadas a PDV\n",
    "print(\"=== ANÁLISE POR TIPO DE PDV ===\")\n",
    "\n",
    "# Procurar colunas relacionadas a PDV/loja\n",
    "store_cols = [col for col in main_df.columns if any(keyword in col.lower() \n",
    "             for keyword in ['pdv', 'store', 'loja', 'premise', 'tipo'])]\n",
    "\n",
    "print(f\"Colunas relacionadas a PDV encontradas: {store_cols}\")\n",
    "\n",
    "# Analisar cada coluna de PDV\n",
    "for col in store_cols:\n",
    "    if main_df[col].dtype == 'object' or main_df[col].nunique() < 100:\n",
    "        print(f\"\\n--- Análise da coluna: {col} ---\")\n",
    "        print(f\"Valores únicos: {main_df[col].nunique()}\")\n",
    "        \n",
    "        value_counts = main_df[col].value_counts()\n",
    "        print(f\"\\nDistribuição:\")\n",
    "        display(value_counts)\n",
    "        \n",
    "        # Visualização se não há muitos tipos\n",
    "        if main_df[col].nunique() <= 15:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            value_counts.plot(kind='bar')\n",
    "            plt.title(f'Distribuição de {col}')\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Frequência')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de vendas por tipo de PDV\n",
    "if qty_col and len(store_cols) > 0:\n",
    "    print(\"=== ANÁLISE DE VENDAS POR TIPO DE PDV ===\")\n",
    "    \n",
    "    # Usar a primeira coluna de tipo de PDV encontrada\n",
    "    store_type_col = None\n",
    "    for col in store_cols:\n",
    "        if main_df[col].dtype == 'object' and main_df[col].nunique() < 20:\n",
    "            store_type_col = col\n",
    "            break\n",
    "    \n",
    "    if store_type_col:\n",
    "        # Vendas por tipo de PDV\n",
    "        store_sales = main_df.groupby(store_type_col)[qty_col].agg([\n",
    "            'sum', 'mean', 'median', 'std', 'count'\n",
    "        ]).round(2)\n",
    "        \n",
    "        store_sales = store_sales.sort_values('sum', ascending=False)\n",
    "        \n",
    "        print(f\"Vendas por {store_type_col}:\")\n",
    "        display(store_sales)\n",
    "        \n",
    "        # Visualizações\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Vendas totais por tipo de PDV\n",
    "        axes[0,0].bar(store_sales.index, store_sales['sum'])\n",
    "        axes[0,0].set_title(f'Vendas Totais por {store_type_col}')\n",
    "        axes[0,0].set_ylabel('Quantidade Total')\n",
    "        axes[0,0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Vendas médias por tipo de PDV\n",
    "        axes[0,1].bar(store_sales.index, store_sales['mean'])\n",
    "        axes[0,1].set_title(f'Vendas Médias por {store_type_col}')\n",
    "        axes[0,1].set_ylabel('Quantidade Média')\n",
    "        axes[0,1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Número de transações por tipo de PDV\n",
    "        axes[1,0].bar(store_sales.index, store_sales['count'])\n",
    "        axes[1,0].set_title(f'Número de Transações por {store_type_col}')\n",
    "        axes[1,0].set_ylabel('Número de Transações')\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Boxplot de distribuição por tipo de PDV\n",
    "        data_for_box = [main_df[main_df[store_type_col] == store_type][qty_col].values \n",
    "                       for store_type in store_sales.index]\n",
    "        \n",
    "        axes[1,1].boxplot(data_for_box, labels=store_sales.index)\n",
    "        axes[1,1].set_title(f'Distribuição de Vendas por {store_type_col}')\n",
    "        axes[1,1].set_ylabel('Quantidade')\n",
    "        axes[1,1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Análise de performance por tipo de PDV\n",
    "        print(f\"\\n=== PERFORMANCE POR TIPO DE PDV ===\")\n",
    "        performance_metrics = pd.DataFrame({\n",
    "            'Tipo_PDV': store_sales.index,\n",
    "            'Vendas_Totais': store_sales['sum'].values,\n",
    "            'Vendas_Medias': store_sales['mean'].values,\n",
    "            'Num_Transacoes': store_sales['count'].values,\n",
    "            'Desvio_Padrao': store_sales['std'].values\n",
    "        })\n",
    "        \n",
    "        # Calcular coeficiente de variação\n",
    "        performance_metrics['Coef_Variacao'] = (performance_metrics['Desvio_Padrao'] / \n",
    "                                              performance_metrics['Vendas_Medias']).round(3)\n",
    "        \n",
    "        display(performance_metrics)\n",
    "    else:\n",
    "        print(\"Nenhuma coluna de tipo de PDV adequada encontrada para análise de vendas.\")"
   ]
  }  },
 
 {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Identificação de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de outliers em colunas numéricas\n",
    "print(\"=== IDENTIFICAÇÃO DE OUTLIERS ===\")\n",
    "\n",
    "numeric_columns = main_df.select_dtypes(include=[np.number]).columns\n",
    "print(f\"Colunas numéricas para análise: {list(numeric_columns)}\")\n",
    "\n",
    "outlier_summary = []\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if main_df[col].nunique() > 10:  # Evitar colunas com poucos valores únicos\n",
    "        print(f\"\\n--- Análise de outliers: {col} ---\")\n",
    "        \n",
    "        # Estatísticas básicas\n",
    "        Q1 = main_df[col].quantile(0.25)\n",
    "        Q3 = main_df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Limites para outliers\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Identificar outliers\n",
    "        outliers_low = main_df[main_df[col] < lower_bound]\n",
    "        outliers_high = main_df[main_df[col] > upper_bound]\n",
    "        total_outliers = len(outliers_low) + len(outliers_high)\n",
    "        \n",
    "        outlier_percentage = (total_outliers / len(main_df)) * 100\n",
    "        \n",
    "        print(f\"Q1: {Q1:.2f}, Q3: {Q3:.2f}, IQR: {IQR:.2f}\")\n",
    "        print(f\"Limites: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "        print(f\"Outliers baixos: {len(outliers_low)} ({len(outliers_low)/len(main_df)*100:.2f}%)\")\n",
    "        print(f\"Outliers altos: {len(outliers_high)} ({len(outliers_high)/len(main_df)*100:.2f}%)\")\n",
    "        print(f\"Total de outliers: {total_outliers} ({outlier_percentage:.2f}%)\")\n",
    "        \n",
    "        # Armazenar resumo\n",
    "        outlier_summary.append({\n",
    "            'Coluna': col,\n",
    "            'Total_Outliers': total_outliers,\n",
    "            'Percentual': outlier_percentage,\n",
    "            'Outliers_Baixos': len(outliers_low),\n",
    "            'Outliers_Altos': len(outliers_high),\n",
    "            'Q1': Q1,\n",
    "            'Q3': Q3,\n",
    "            'IQR': IQR\n",
    "        })\n",
    "\n",
    "# Resumo de outliers\n",
    "if outlier_summary:\n",
    "    outlier_df = pd.DataFrame(outlier_summary)\n",
    "    outlier_df = outlier_df.sort_values('Percentual', ascending=False)\n",
    "    \n",
    "    print(\"\\n=== RESUMO DE OUTLIERS ===\")\n",
    "    display(outlier_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização de outliers\n",
    "if len(numeric_columns) > 0:\n",
    "    print(\"=== VISUALIZAÇÃO DE OUTLIERS ===\")\n",
    "    \n",
    "    # Selecionar até 4 colunas numéricas mais relevantes\n",
    "    cols_to_plot = list(numeric_columns)[:4]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, len(cols_to_plot), figsize=(5*len(cols_to_plot), 10))\n",
    "    \n",
    "    if len(cols_to_plot) == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    for i, col in enumerate(cols_to_plot):\n",
    "        # Boxplot\n",
    "        axes[0, i].boxplot(main_df[col].dropna())\n",
    "        axes[0, i].set_title(f'Boxplot - {col}')\n",
    "        axes[0, i].set_ylabel('Valores')\n",
    "        \n",
    "        # Histograma\n",
    "        axes[1, i].hist(main_df[col].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "        axes[1, i].set_title(f'Histograma - {col}')\n",
    "        axes[1, i].set_xlabel('Valores')\n",
    "        axes[1, i].set_ylabel('Frequência')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Análise específica da coluna de quantidade (se existir)\n",
    "    if qty_col and qty_col in main_df.columns:\n",
    "        print(f\"\\n=== ANÁLISE DETALHADA DE OUTLIERS - {qty_col} ===\")\n",
    "        \n",
    "        qty_data = main_df[qty_col].dropna()\n",
    "        \n",
    "        # Estatísticas detalhadas\n",
    "        print(f\"Estatísticas de {qty_col}:\")\n",
    "        print(f\"Mínimo: {qty_data.min()}\")\n",
    "        print(f\"Máximo: {qty_data.max()}\")\n",
    "        print(f\"Média: {qty_data.mean():.2f}\")\n",
    "        print(f\"Mediana: {qty_data.median():.2f}\")\n",
    "        print(f\"Desvio padrão: {qty_data.std():.2f}\")\n",
    "        \n",
    "        # Percentis\n",
    "        percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]\n",
    "        print(f\"\\nPercentis:\")\n",
    "        for p in percentiles:\n",
    "            value = qty_data.quantile(p/100)\n",
    "            print(f\"P{p}: {value:.2f}\")\n",
    "        \n",
    "        # Visualização detalhada\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        \n",
    "        # Histograma completo\n",
    "        axes[0].hist(qty_data, bins=100, alpha=0.7, edgecolor='black')\n",
    "        axes[0].set_title(f'Distribuição Completa - {qty_col}')\n",
    "        axes[0].set_xlabel('Quantidade')\n",
    "        axes[0].set_ylabel('Frequência')\n",
    "        \n",
    "        # Histograma sem outliers extremos (99% dos dados)\n",
    "        p99 = qty_data.quantile(0.99)\n",
    "        qty_filtered = qty_data[qty_data <= p99]\n",
    "        axes[1].hist(qty_filtered, bins=50, alpha=0.7, edgecolor='black', color='orange')\n",
    "        axes[1].set_title(f'Distribuição (até P99) - {qty_col}')\n",
    "        axes[1].set_xlabel('Quantidade')\n",
    "        axes[1].set_ylabel('Frequência')\n",
    "        \n",
    "        # Boxplot detalhado\n",
    "        axes[2].boxplot(qty_data, patch_artist=True)\n",
    "        axes[2].set_title(f'Boxplot Detalhado - {qty_col}')\n",
    "        axes[2].set_ylabel('Quantidade')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  }  },
 
 {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Análise de Correlações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de correlações entre variáveis numéricas\n",
    "print(\"=== ANÁLISE DE CORRELAÇÕES ===\")\n",
    "\n",
    "# Selecionar apenas colunas numéricas com variabilidade\n",
    "numeric_cols_for_corr = []\n",
    "for col in numeric_columns:\n",
    "    if main_df[col].nunique() > 1 and main_df[col].std() > 0:\n",
    "        numeric_cols_for_corr.append(col)\n",
    "\n",
    "print(f\"Colunas numéricas para análise de correlação: {numeric_cols_for_corr}\")\n",
    "\n",
    "if len(numeric_cols_for_corr) > 1:\n",
    "    # Calcular matriz de correlação\n",
    "    correlation_matrix = main_df[numeric_cols_for_corr].corr()\n",
    "    \n",
    "    print(\"\\nMatriz de correlação:\")\n",
    "    display(correlation_matrix.round(3))\n",
    "    \n",
    "    # Visualização da matriz de correlação\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "    plt.title('Matriz de Correlação - Variáveis Numéricas')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identificar correlações mais fortes\n",
    "    print(\"\\n=== CORRELAÇÕES MAIS FORTES ===\")\n",
    "    \n",
    "    # Extrair correlações (excluindo diagonal)\n",
    "    correlations = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            var1 = correlation_matrix.columns[i]\n",
    "            var2 = correlation_matrix.columns[j]\n",
    "            corr_value = correlation_matrix.iloc[i, j]\n",
    "            correlations.append({\n",
    "                'Variavel_1': var1,\n",
    "                'Variavel_2': var2,\n",
    "                'Correlacao': corr_value,\n",
    "                'Correlacao_Abs': abs(corr_value)\n",
    "            })\n",
    "    \n",
    "    corr_df = pd.DataFrame(correlations)\n",
    "    corr_df = corr_df.sort_values('Correlacao_Abs', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 correlações mais fortes:\")\n",
    "    display(corr_df.head(10)[['Variavel_1', 'Variavel_2', 'Correlacao']])\n",
    "    \n",
    "    # Identificar correlações problemáticas (multicolinearidade)\n",
    "    high_corr = corr_df[corr_df['Correlacao_Abs'] > 0.8]\n",
    "    if len(high_corr) > 0:\n",
    "        print(\"\\n⚠️ ATENÇÃO: Correlações muito altas detectadas (>0.8):\")\n",
    "        display(high_corr[['Variavel_1', 'Variavel_2', 'Correlacao']])\n",
    "        print(\"Considere remover uma das variáveis para evitar multicolinearidade.\")\n",
    "else:\n",
    "    print(\"Não há colunas numéricas suficientes para análise de correlação.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Insights e Recomendações para Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo dos insights encontrados\n",
    "print(\"=== RESUMO DOS INSIGHTS PARA FEATURE ENGINEERING ===\")\n",
    "\n",
    "insights = []\n",
    "\n",
    "# Insights sobre dados temporais\n",
    "if date_col:\n",
    "    insights.append(\"✅ TEMPORAL: Dados temporais identificados - criar features de sazonalidade\")\n",
    "    if 'semana' in df_temporal.columns:\n",
    "        cv_semanal = df_temporal.groupby('semana')[qty_col].sum().std() / df_temporal.groupby('semana')[qty_col].sum().mean()\n",
    "        if cv_semanal > 0.3:\n",
    "            insights.append(f\"📈 SAZONALIDADE: Alta variabilidade semanal (CV={cv_semanal:.3f}) - importante para o modelo\")\n",
    "        else:\n",
    "            insights.append(f\"📊 SAZONALIDADE: Variabilidade semanal moderada (CV={cv_semanal:.3f})\")\n",
    "else:\n",
    "    insights.append(\"❌ TEMPORAL: Nenhuma coluna de data identificada - verificar dados\")\n",
    "\n",
    "# Insights sobre categorias de produto\n",
    "if len(product_cols) > 0:\n",
    "    insights.append(f\"✅ PRODUTOS: {len(product_cols)} colunas de produto identificadas - usar para encoding\")\n",
    "    for col in product_cols[:2]:  # Primeiras 2 colunas\n",
    "        unique_count = main_df[col].nunique()\n",
    "        if unique_count < 50:\n",
    "            insights.append(f\"🏷️ CATEGORIA: {col} tem {unique_count} categorias - adequado para one-hot encoding\")\n",
    "        else:\n",
    "            insights.append(f\"🏷️ CATEGORIA: {col} tem {unique_count} categorias - considerar target encoding\")\n",
    "else:\n",
    "    insights.append(\"❌ PRODUTOS: Nenhuma coluna de categoria identificada\")\n",
    "\n",
    "# Insights sobre PDVs\n",
    "if len(store_cols) > 0:\n",
    "    insights.append(f\"✅ PDV: {len(store_cols)} colunas de PDV identificadas\")\n",
    "    for col in store_cols[:2]:  # Primeiras 2 colunas\n",
    "        unique_count = main_df[col].nunique()\n",
    "        if unique_count < 20:\n",
    "            insights.append(f\"🏪 TIPO_PDV: {col} tem {unique_count} tipos - criar features por tipo\")\n",
    "else:\n",
    "    insights.append(\"❌ PDV: Nenhuma coluna de PDV identificada\")\n",
    "\n",
    "# Insights sobre outliers\n",
    "if qty_col and outlier_summary:\n",
    "    qty_outliers = next((item for item in outlier_summary if item['Coluna'] == qty_col), None)\n",
    "    if qty_outliers and qty_outliers['Percentual'] > 5:\n",
    "        insights.append(f\"⚠️ OUTLIERS: {qty_outliers['Percentual']:.1f}% outliers em quantidade - aplicar winsorização\")\n",
    "    elif qty_outliers:\n",
    "        insights.append(f\"✅ OUTLIERS: {qty_outliers['Percentual']:.1f}% outliers em quantidade - nível aceitável\")\n",
    "\n",
    "# Insights sobre dados faltantes\n",
    "total_missing = main_df.isnull().sum().sum()\n",
    "if total_missing > 0:\n",
    "    missing_percent = (total_missing / (len(main_df) * len(main_df.columns))) * 100\n",
    "    if missing_percent > 10:\n",
    "        insights.append(f\"⚠️ MISSING: {missing_percent:.1f}% dados faltantes - estratégia de imputação necessária\")\n",
    "    else:\n",
    "        insights.append(f\"✅ MISSING: {missing_percent:.1f}% dados faltantes - nível baixo\")\n",
    "else:\n",
    "    insights.append(\"✅ MISSING: Nenhum dado faltante encontrado\")\n",
    "\n",
    "# Insights sobre correlações\n",
    "if len(numeric_cols_for_corr) > 1:\n",
    "    high_corr_count = len(corr_df[corr_df['Correlacao_Abs'] > 0.8])\n",
    "    if high_corr_count > 0:\n",
    "        insights.append(f\"⚠️ CORRELAÇÃO: {high_corr_count} pares com correlação >0.8 - risco de multicolinearidade\")\n",
    "    else:\n",
    "        insights.append(\"✅ CORRELAÇÃO: Sem correlações problemáticas detectadas\")\n",
    "\n",
    "# Exibir insights\n",
    "for insight in insights:\n",
    "    print(insight)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMENDAÇÕES PARA PRÓXIMAS ETAPAS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "recommendations = [\n",
    "    \"1. 🕒 Criar features temporais: semana, mês, trimestre, sazonalidade\",\n",
    "    \"2. 📊 Implementar features de lag: vendas das últimas 1, 2, 4, 8 semanas\",\n",
    "    \"3. 📈 Calcular médias móveis e estatísticas rolling por PDV/produto\",\n",
    "    \"4. 🏷️ Aplicar encoding adequado para variáveis categóricas\",\n",
    "    \"5. 🎯 Criar features de performance histórica por PDV e categoria\",\n",
    "    \"6. 🧹 Implementar tratamento de outliers (winsorização ou capping)\",\n",
    "    \"7. 🔄 Desenvolver estratégia de validação temporal (walk-forward)\",\n",
    "    \"8. 📋 Criar features de ranking e percentis por categoria\"\n",
    "]\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sumário Executivo da EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar sumário executivo\n",
    "print(\"=== SUMÁRIO EXECUTIVO DA ANÁLISE EXPLORATÓRIA ===\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Informações básicas do dataset\n",
    "print(f\"📊 DATASET PRINCIPAL:\")\n",
    "print(f\"   • Registros: {len(main_df):,}\")\n",
    "print(f\"   • Colunas: {len(main_df.columns)}\")\n",
    "print(f\"   • Memória: {main_df.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Qualidade dos dados\n",
    "missing_total = main_df.isnull().sum().sum()\n",
    "duplicates_total = main_df.duplicated().sum()\n",
    "print(f\"\\n🔍 QUALIDADE DOS DADOS:\")\n",
    "print(f\"   • Valores faltantes: {missing_total:,} ({missing_total/(len(main_df)*len(main_df.columns))*100:.1f}%)\")\n",
    "print(f\"   • Registros duplicados: {duplicates_total:,} ({duplicates_total/len(main_df)*100:.1f}%)\")\n",
    "\n",
    "# Informações temporais\n",
    "if date_col:\n",
    "    date_range = f\"{df_temporal[date_col].min().strftime('%Y-%m-%d')} a {df_temporal[date_col].max().strftime('%Y-%m-%d')}\"\n",
    "    weeks_count = df_temporal['semana'].nunique() if 'semana' in df_temporal.columns else 'N/A'\n",
    "    print(f\"\\n📅 INFORMAÇÕES TEMPORAIS:\")\n",
    "    print(f\"   • Período: {date_range}\")\n",
    "    print(f\"   • Semanas únicas: {weeks_count}\")\n",
    "else:\n",
    "    print(f\"\\n❌ INFORMAÇÕES TEMPORAIS: Não identificadas\")\n",
    "\n",
    "# Informações de vendas\n",
    "if qty_col:\n",
    "    total_sales = main_df[qty_col].sum()\n",
    "    avg_sales = main_df[qty_col].mean()\n",
    "    print(f\"\\n💰 INFORMAÇÕES DE VENDAS:\")\n",
    "    print(f\"   • Total vendido: {total_sales:,.0f} unidades\")\n",
    "    print(f\"   • Média por transação: {avg_sales:.1f} unidades\")\n",
    "    print(f\"   • Transações: {len(main_df):,}\")\n",
    "\n",
    "# Diversidade de produtos e PDVs\n",
    "if product_cols:\n",
    "    main_product_col = product_cols[0]\n",
    "    product_count = main_df[main_product_col].nunique()\n",
    "    print(f\"\\n🏷️ DIVERSIDADE DE PRODUTOS:\")\n",
    "    print(f\"   • Produtos únicos: {product_count:,}\")\n",
    "\n",
    "if store_cols:\n",
    "    main_store_col = store_cols[0]\n",
    "    store_count = main_df[main_store_col].nunique()\n",
    "    print(f\"\\n🏪 DIVERSIDADE DE PDVs:\")\n",
    "    print(f\"   • PDVs únicos: {store_count:,}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"✅ ANÁLISE EXPLORATÓRIA CONCLUÍDA\")\n",
    "print(f\"📋 Próximo passo: Feature Engineering (Task 5)\")\n",
    "print(f\"=\"*60)"
   ]
  }  }
 ],

 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}